{"componentChunkName":"component---src-templates-blog-post-blog-post-jsx","path":"/2021/08/migrating-desktop-apps-to-the-cloud","result":{"data":{"markdownRemark":{"html":"<p>Recently, I had the good fortune to work on a very technically interesting and well-architected project which gives an excellent demonstration of what can be achieved when using multiple cloud services in parallel.</p>\n<p>A client came to us with a state-of-the-art modelling system which had been designed by industry-leading domain experts. The techniques used in the modelling software meant that, for its use-case, the software could run simulations in an order of magnitude less time than the competing solutions.</p>\n<p>Despite being an industry-leading scientific model, the delivery of that solution through software was holding the product back from its full potential. The software was written using a Python GUI which was distributed as source code directly to users. The model could take hours to run and if it failed (often due to user error from the complexity of input), the user would only discover this error after returning to the program after several hours, only to need to fix the error and rerun the test case. Often multiple test cases would need to be run and the desktop application did not deal with concurrent runs (as the software is processor intensive).</p>\n<h1>The Mission</h1>\n<p>We wanted to bring this software to the market as an industry-leading product in terms of scientific modelling <em>and</em> software engineering. We would leverage cloud services to solve some of the disadvantages of the desktop implementation.</p>\n<p>Here are some of the main things we wanted to add to the current functionality of the model:</p>\n<ol>\n<li>\n<p>Improved distribution by migrating to the cloud:</p>\n<ul>\n<li>Easier on-boarding (no software installation - just login).</li>\n<li>Improved security (no user access to source code).</li>\n<li>Run the model from any computer.</li>\n</ul>\n</li>\n<li>\n<p>Increased scalability:</p>\n<ul>\n<li>Run the model concurrently as many times as needed to save users' time.</li>\n</ul>\n</li>\n<li>\n<p>Improved user experience:</p>\n<ul>\n<li>More data validation rules to ensure valid input.</li>\n<li>Email notifications for when the run is complete.</li>\n</ul>\n</li>\n</ol>\n<h1>The Challenges</h1>\n<p>This migration was tricky in several respects, but three stand out as the most awkward in terms of cloud migration:</p>\n<ol>\n<li>\n<p>The model utilises intermediate output files to save data between steps in the run. This decision was initially made from a scientific standpoint - a modeller might want to check some intermediate output - and stuck as a design pattern of the software.</p>\n</li>\n<li>\n<p>The model inherently called <code class=\"language-text\">os.system</code> in many places to run commands on a specific library. This library required GNU Fortran to be installed (not something you see in your everyday web application).</p>\n</li>\n<li>\n<p>The model had significant complexity, and re-engineering it came with significant risks to the validity of the results.</p>\n</li>\n</ol>\n<p>These constraints meant that we needed:</p>\n<ol>\n<li>\n<p>To run the model code on a system with the same environment as the modellers' computers.</p>\n</li>\n<li>\n<p>A way to persist intermediate files in the cloud between different runs.</p>\n</li>\n</ol>\n<h1>How did we do it?</h1>\n<p>The architect of the project drew a handy architecture diagram to illustrate exactly how we achieved these goals and I'll go through the steps that were took along the way.</p>\n<br>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/dc19244b3af00913f9b26351a651950a/58213/task-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACCElEQVQoz3VT227TQBD1z/APfAUfwEfwzFOl8sZrJB64CHgIFAXBE6hCKFLvLSioagpqGuJc3NRu4iZxHWfvuwd5122TACONNZrdObM+c8ZDYcYYzMfznluWUGhp/jpfrvXmE8lkDEYpbs3luWL4ePAancFpAaAXgHObZVNbawGVchdKlS18/d60sRASSkrktYNxhAfl+6jWP9szbfQNmNKu9sn7TazvN+AZraGVsskfJwHa/djGneAM58MIhBLnZApKCbjgkEpBSgmttPXcDk8DdKOxe6FrqzFvcRJj299Akibwux3XpB9iPBlBQ4NyCrPIjOOQ7+2D12o2NzuqgHZ2bMwFgzYKjCskgx7k0Utc+btgQiPngUgG0QyhfkfIn0KaXyDCGrzRm3eIKx8sSLP8EBc7b5ETkGUplBI2Tnt19B/fxWTrGUTejBLMINBeLSN45O4HpXsYfVqFJzm/mWxKBCh3fHIpwCV3A2IE7cNdZKOLgh1lf9tvttBquMkHvRBxnMCbPH2Bq7XKAhm5EoQQoIKBSg4iOBIFMACMUzDBrGtjoIua4KSKob8Jj/UC8PPIAWm9KFgpMB2GYJMQ6V4JelC/Vr5taj+FJv1vrxDV126nvLwpVp8kRdb5Cd4/RnflDpLt50VjtSTsuU25ftW/VirntvHrGOn4Eq2DKi7P2v9dPVOI/Q/WG5LJQt9ZbwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"task architecture\"\n        title=\"task architecture\"\n        src=\"/blog/static/dc19244b3af00913f9b26351a651950a/5a190/task-architecture.png\"\n        srcset=\"/blog/static/dc19244b3af00913f9b26351a651950a/772e8/task-architecture.png 200w,\n/blog/static/dc19244b3af00913f9b26351a651950a/e17e5/task-architecture.png 400w,\n/blog/static/dc19244b3af00913f9b26351a651950a/5a190/task-architecture.png 800w,\n/blog/static/dc19244b3af00913f9b26351a651950a/58213/task-architecture.png 902w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<br>\n<h2>Dockerize the project: AWS Fargate + ECR + VPC</h2>\n<p>The first step was to create an environment to run the model; Docker was designed to solve this very problem (\"well, it worked when it ran on <em>my</em> computer!\").</p>\n<p>A new container was defined and the dependencies for the project were resolved. We used Poetry to manage the Python libraries.</p>\n<p>We used Docker <code class=\"language-text\">volumes</code> for the project so that any intermediate files created within the container are easy to inspect and validate. A <code class=\"language-text\">docker-compose.yml</code> file was also written to simplify bringing the container up and down. We also used <code class=\"language-text\">docker-compose</code> to pass in environment variables from our <code class=\"language-text\">.env</code> files.</p>\n<p>There are several discrete tasks that the model can run to prepare different output files. We defined a a new entry point to the program which reads the context from the environment variables. The main thing of interest for us now is to define which task we want to run - which we pass in as an environment variable.</p>\n<p>Now that we've passed in the task we want to run, we can define what functions we want to run for each task. We define a <code class=\"language-text\">TASK_DEFINITIONS</code> variable which holds several handlers: <code class=\"language-text\">failure</code> (the model can be temperamental to user input, and we need to handle failure explicitly), <code class=\"language-text\">preprocessor</code>, <code class=\"language-text\">executor</code> (where we will run the main task), and <code class=\"language-text\">postprocessor</code>. I will come back to the pre- and post-processors in the next section.</p>\n<p>The executor is simply a function which makes a call to the model source code to execute the desired task. This means we can define an <code class=\"language-text\">env</code> variable - <code class=\"language-text\">TASK_NAME</code> - and run the model locally by accessing bash from within the container (<code class=\"language-text\">docker-compose up -d &amp;&amp; docker-compose exec &lt;image-name> bash &amp;&amp; python main.py</code>).</p>\n<p>The container can now be migrated to the cloud so that the model code can be accessed from anywhere in the world without the need to manually distribute the source code.</p>\n<p>The image is registered with AWS Elastic Container Registry (ECR) where the container can be stored, ready to be accessed (this is <code class=\"language-text\">Model registry</code> in our architecture diagram). A Virtual Private Cloud (VPC) is configured to accept HTTP requests to trigger an AWS Fargate task. You can see Fargate (<code class=\"language-text\">Model task</code>) sitting at the centre of our architecture diagram within the <code class=\"language-text\">VPC</code> in <code class=\"language-text\">ECS</code>.</p>\n<p>Fargate manages the execution of tasks and will deal with spinning up an instance from our Docker image (registered in ECR) without the need for us to manage any EC2 containers.</p>\n<p>We also set up Cloudwatch to create logs for each run to help us and the modellers debug issues with the model.</p>\n<h2>Syncing input and output directories: AWS S3</h2>\n<p>Until now, the model works in the cloud so long as the required input files are present in the file system of the image; however, input files are specific to the context of a run, and if we want to get meaningful output then we need to pass in the correct input files at the point that we execute a task.</p>\n<p>For each task, a new instance of the image is spun-up, so any intermediate files are lost when the container is pulled down and the context is lost.</p>\n<p>We set up an S3 bucket to store the inputs and outputs of tasks, which means that the files which we expect to persist between different tasks can be synced between permanent storage from task to task.</p>\n<p>Each task saved its output to a different directory, so within our <code class=\"language-text\">TASK_DEFINITIONS</code>, we define an <code class=\"language-text\">input_dir</code>, from which we would pull the contents from S3 and a number of <code class=\"language-text\">output_dirs</code> to which we would push the output of the model. These outputs might be downloaded by the user or used by another task as input files. This syncing of files happens in the <code class=\"language-text\">preprocess</code> and <code class=\"language-text\">postprocess</code> of each task.</p>\n<h2>Data model: AWS DynamoDB</h2>\n<p>The model not only depends on input files, but also on a range of input parameters that the user can define. Each model run entity has an entity within a DynamoDB table (in practice we have multiple types of entity available to the user). The database follows the single-table design described in books like <code class=\"language-text\">The DynamoDB Handbook</code> by Alex BeBrie and uses <code class=\"language-text\">dynamodb-toolbox</code> to model objects. The user can interact with these entities via the web application to set properties of the run.</p>\n<p>We created a corresponding manager for each type of entity in the database (e.g. <code class=\"language-text\">RunManager</code>) which can get the entity and also perform simple updates on text fields (send error messages back etc.). The retrieved object then overrides the variables which were previously being retrieved from a local <code class=\"language-text\">INPUT_PARAMETERS.txt</code> file, which describes some generic run.</p>\n<p>The model is now no-longer dependent on a local file for input of discrete variables; but how does the user configure those variables to get a final valid output to their desired parameters.</p>\n<h2>Web Application: AWS Lambda + Next.js</h2>\n<p>It's now time to plug the main pieces together. We developed a Next.js frontend to interface with the remote model. This comes with all the advantages of developing UI using technologies that were designed to handle UI (i.e. not Python!). This meant our input methods were more interactive and our outputs were more visual, as we were able to leverage existing libraries.</p>\n<p>We added validation rules to the inputs (with warning messages) to prevent the user running the model with parameters that don't make sense before saving these parameters to DynamoDB. We accept input files as upload fields in a form, which had some pre-processing steps to validate the values within the files were within the restrictions before uploading to S3.</p>\n<p>We used AWS Lambdas to build our backend REST API. Serverless offers flexible scaling, has a nice integration to the back-office tool we created in Retool, and we didn't care about cold-starts as the model already took hours to run.</p>\n<p>Another advantage of using Lambda was that we can use AWS CloudFormation (infrastructure as code solution) to provision our infrastructure from a <code class=\"language-text\">serverless.ts</code> file which defines all the lambdas and resources we would need. If we wanted to create a new environment, running <code class=\"language-text\">sls deploy</code> would provision the resources defined in the file instead of needing to interact with the AWS console.</p>\n<h2>Notifying model outcomes: AWS Event Bridge + SES</h2>\n<p>Until now, a specific model task can be triggered to run in the cloud where it will sync its the relevant input files, pull the correct input parameters, run the task, and sync the output files back to S3. Now we just need to notify the user of the outcome of the model (success + results/ error + logs) and show them the results.</p>\n<p>In the model, we defined a <code class=\"language-text\">RunEventsManager</code> which could send an event via AWS Event Bridge when the <code class=\"language-text\">postprocess</code> or <code class=\"language-text\">failure</code> handler was completed in the model. We added a Lambda in our backend which was triggered by an event bridge event (because we were using CloudFormation we just needed to define the event that triggers the lambda in <code class=\"language-text\">serverless.ts</code>).</p>\n<p>When an event occurs, the Lambda uses AWS SES client to send an email to the user, which notifies them immediately when they can check their results - there's no need to come back and check the model hasn't failed every 15 mintes anymore!</p>\n<h1>What we acheived</h1>\n<p>By the end of the project we could trigger a task in Fargate to run a model from from our frontend. We can run as many tasks as the user requires concurrently and we will get notified by email of any updates to our runs which require our attention.</p>","frontmatter":{"date":"August 21, 2021","slug":"2021/08/migrating-desktop-apps-to-the-cloud","title":"Migrating desktop apps to the cloud with AWS","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA9ElEQVQY04XMPS8DARzH8d/1JkQTU0Ni6yASJMaKxcLQyWpVEa/A0kRKhKG487B5Md4Bi8XDYGpET9z1/k+n/YuV4b7jd/gAf8rzAbIsxWfSHxORTSJqqOqyiCwx8xpT3ny+vwvfXh7xlXygNBUKlAdg5jkTigrTC2HuivC1MO1pMWqL+7QMHTr0oBQ098DcIfa9qO4tdV8RG26wjZqp+6yk77vae6hb/wmavJaDdAjwMZB3UJdTNGwL1d/v7rCbiRqdV7blBOvUBqiDcpAvx9E7qyG7nQ8tClYtDveLuHJUxOGBRdjhLhbSaHIquZpBFlf/AT+JUpai18VmwwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/821da/thumbnail.png","srcSet":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/22719/thumbnail.png 185w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/b91c6/thumbnail.png 370w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/821da/thumbnail.png 740w","sizes":"(min-width: 740px) 740px, 100vw"},"sources":[{"srcSet":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/6b17e/thumbnail.avif 185w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/097b8/thumbnail.avif 370w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/3950a/thumbnail.avif 740w","type":"image/avif","sizes":"(min-width: 740px) 740px, 100vw"},{"srcSet":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/4d5e6/thumbnail.webp 185w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/bb639/thumbnail.webp 370w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/4baf0/thumbnail.webp 740w","type":"image/webp","sizes":"(min-width: 740px) 740px, 100vw"}]},"width":800,"height":196.75675675675677}}}}}},"pageContext":{"slug":"2021/08/migrating-desktop-apps-to-the-cloud"}},"staticQueryHashes":[]}