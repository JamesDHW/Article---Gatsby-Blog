{"componentChunkName":"component---src-templates-blog-post-blog-post-jsx","path":"/2021/08/migrating-desktop-apps-to-the-cloud","result":{"data":{"markdownRemark":{"html":"<p><strong>TLDR:</strong> <em>Recently, I had the good fortune to work on a very technically interesting and well-architected project. In this article, I explain how your cloud architecture can handle awkward constraints placed upon it when bringing a product to the cloud. This case study gives an excellent demonstration of what can be achieved when we use multiple cloud services in parallel.</em></p>\n<p>A client approached us with a state-of-the-art modelling system which had been designed by industry-leading domain experts. The techniques used in the modelling software meant that, for its use-case, the software could run simulations in an order of magnitude less time than the competing solutions.</p>\n<p>Despite being an industry-leading scientific model, the delivery of that solution through software was holding the product back from its full potential. The software interface was a Python GUI, and was distributed as source code directly to users. The model could take hours to run and if it failed (often due to invalid input), the modeller would only discover this error on returning to the program after several hours away, only to correct the error and rerun the test case. Often, multiple variations need to be simulated, however the desktop application couldn't execute multiple runs concurrently (as the software is limited by the processing power of the machine it runs on).</p>\n<h1>The mission</h1>\n<p>We wanted to bring this software to the market as an industry-leading product in terms of scientific modelling <em>and</em> software engineering. We would leverage cloud services to solve some of the disadvantages of the desktop implementation.</p>\n<p>Here are some of the main things we wanted to add to the current functionality of the model:</p>\n<ol>\n<li>\n<p>Improved distribution by migrating to the cloud:</p>\n<ul>\n<li>Easier on-boarding (no software installation - just login).</li>\n<li>Improved security (no user access to source code).</li>\n<li>Run the model from any computer.</li>\n</ul>\n</li>\n<li>\n<p>Increased scalability:</p>\n<ul>\n<li>Execute multiple model runs concurrently to save users' time.</li>\n</ul>\n</li>\n<li>\n<p>Improved user experience:</p>\n<ul>\n<li>More data validation rules to prevent failed model runs.</li>\n<li>Email notifications for when the model has completed a run.</li>\n</ul>\n</li>\n</ol>\n<h1>The challenges</h1>\n<p>This migration was tricky in several respects, but three stand out as the most awkward for the cloud migration:</p>\n<ol>\n<li>\n<p>The model utilises intermediate output files to save data between steps in the run. This decision was initially made from a scientific standpoint - a modeller might want to check some intermediate output - and had stuck as a design pattern across the software.</p>\n</li>\n<li>\n<p>The model inherently calls <code class=\"language-text\">os.system</code> in many places to run commands on a specific library. This library also had a dependency on GNU Fortran (not something you see in your everyday web application).</p>\n</li>\n<li>\n<p>The model code has significant complexity, and re-engineering it came with significant risks to the validity of the results (re-engineering the code is out of the question).</p>\n</li>\n</ol>\n<p>These constraints meant that we needed:</p>\n<ol>\n<li>\n<p>To run the model code on a system with the same environment as the modellers' computers.</p>\n</li>\n<li>\n<p>A way to persist intermediate files in the cloud between different runs.</p>\n</li>\n</ol>\n<h1>How did we do it?</h1>\n<p>The architect of the project drew a handy architecture diagram to illustrate exactly how we achieved these goals and I'll go through the steps that we took along the way.</p>\n<br>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/dc19244b3af00913f9b26351a651950a/58213/task-architecture.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACCElEQVQoz3VT227TQBD1z/APfAUfwEfwzFOl8sZrJB64CHgIFAXBE6hCKFLvLSioagpqGuJc3NRu4iZxHWfvuwd5122TACONNZrdObM+c8ZDYcYYzMfznluWUGhp/jpfrvXmE8lkDEYpbs3luWL4ePAancFpAaAXgHObZVNbawGVchdKlS18/d60sRASSkrktYNxhAfl+6jWP9szbfQNmNKu9sn7TazvN+AZraGVsskfJwHa/djGneAM58MIhBLnZApKCbjgkEpBSgmttPXcDk8DdKOxe6FrqzFvcRJj299Akibwux3XpB9iPBlBQ4NyCrPIjOOQ7+2D12o2NzuqgHZ2bMwFgzYKjCskgx7k0Utc+btgQiPngUgG0QyhfkfIn0KaXyDCGrzRm3eIKx8sSLP8EBc7b5ETkGUplBI2Tnt19B/fxWTrGUTejBLMINBeLSN45O4HpXsYfVqFJzm/mWxKBCh3fHIpwCV3A2IE7cNdZKOLgh1lf9tvttBquMkHvRBxnMCbPH2Bq7XKAhm5EoQQoIKBSg4iOBIFMACMUzDBrGtjoIua4KSKob8Jj/UC8PPIAWm9KFgpMB2GYJMQ6V4JelC/Vr5taj+FJv1vrxDV126nvLwpVp8kRdb5Cd4/RnflDpLt50VjtSTsuU25ftW/VirntvHrGOn4Eq2DKi7P2v9dPVOI/Q/WG5LJQt9ZbwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"cloud infrastructure architecture diagram\"\n        title=\"cloud infrastructure architecture diagram\"\n        src=\"/blog/static/dc19244b3af00913f9b26351a651950a/5a190/task-architecture.png\"\n        srcset=\"/blog/static/dc19244b3af00913f9b26351a651950a/772e8/task-architecture.png 200w,\n/blog/static/dc19244b3af00913f9b26351a651950a/e17e5/task-architecture.png 400w,\n/blog/static/dc19244b3af00913f9b26351a651950a/5a190/task-architecture.png 800w,\n/blog/static/dc19244b3af00913f9b26351a651950a/58213/task-architecture.png 902w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<br>\n<h2>Dockerize the project: Fargate + ECR + VPC</h2>\n<p>The first step was to create an environment to run the model; Docker was designed to solve this very problem (\"well, it worked when it ran on <em>my</em> computer!\").</p>\n<p>A new container was defined and the dependencies for the project were resolved. We used <a href=\"https://python-poetry.org/\">Poetry</a> to manage the Python libraries.</p>\n<p>We used Docker <code class=\"language-text\">volumes</code> for the project so that any intermediate files created within the container are synced with our outer file systems to inspect and validate. A <code class=\"language-text\">docker-compose.yml</code> file was also written to simplify bringing the container up and down. We also used <code class=\"language-text\">docker-compose</code> to pass in environment variables from a <code class=\"language-text\">.env</code> file.</p>\n<p>There are several discrete tasks that the model can run which each generate different types of output files. We defined a new entry point to the program which reads the context of the task from the environment variables; we dictated which task to run with the <code class=\"language-text\">TASK_NAME</code> environemt variable.</p>\n<p>Now that we've passed in the task we want to run, we can define what functions we want to run for each task. We define a <code class=\"language-text\">TASK_DEFINITIONS</code> variable which stores handlers for each task: <code class=\"language-text\">preprocessor</code>, <code class=\"language-text\">executor</code> (where we will run the main task), <code class=\"language-text\">postprocessor</code>, and <code class=\"language-text\">failure</code> (the model can be temperamental to user input, and we need to handle failure explicitly). I will come back to the pre- and post-processors in the next section.</p>\n<p>The executor is simply a function which makes a call to the model source code to execute the desired task. This means we can define an <code class=\"language-text\">env</code> variable - <code class=\"language-text\">TASK_NAME</code> - and run the model locally by accessing bash from within the container (<code class=\"language-text\">docker-compose up -d &amp;&amp; docker-compose exec &lt;image-name> bash &amp;&amp; python main.py</code>).</p>\n<p>The container can now be migrated to the cloud so that the model code can be accessed from anywhere in the world without the need to manually distribute the source code.</p>\n<p>The image is registered with AWS Elastic Container Registry (<a href=\"https://aws.amazon.com/ecr/\">ECR</a>) where the container can be stored and accessed (this is <code class=\"language-text\">Model registry</code> in our architecture diagram). A Virtual Private Cloud (<a href=\"https://aws.amazon.com/vpc/\">VPC</a>) is configured to accept HTTP requests to trigger an <a href=\"https://aws.amazon.com/fargate/\">AWS Fargate</a> task. You can see Fargate (<code class=\"language-text\">Model task</code>) sitting at the centre of our architecture diagram within the VPC in ECS.</p>\n<p>Fargate manages the execution of tasks and will deal with spinning up an instance from our Docker image (registered in ECR) without the need for us to manage any EC2 containers.</p>\n<p>We also set up <a href=\"https://aws.amazon.com/cloudwatch/\">Cloudwatch</a> to create logs for each run to help us and the modellers debug issues with the model.</p>\n<h2>Syncing input and output directories: S3</h2>\n<p>Until now, the model works in the cloud so long as the required input files are present in the file system of the image; however, input files are specific to the context of a run, and if we want to get meaningful output then we need to pass in the correct input files at the point that we execute a task.</p>\n<p>For each task, a new instance of the image is spun-up, so any intermediate files are lost when the container is pulled down and the context is lost.</p>\n<p>We set up an <a href=\"https://aws.amazon.com/s3/\">S3</a> bucket to store the inputs and outputs of tasks, which means that the files which we expect to persist between different tasks can be synced between permanent storage from task to task.</p>\n<p>Each task saved its output to a different directory, so within our <code class=\"language-text\">TASK_DEFINITIONS</code>, we define an <code class=\"language-text\">input_dir</code>, from which we would pull the contents from S3 and a number of <code class=\"language-text\">output_dirs</code> to which we would push the output of the model. These outputs might be downloaded by the user or used by another task as input files. This syncing of files happens in the <code class=\"language-text\">preprocess</code> and <code class=\"language-text\">postprocess</code> of each task.</p>\n<h2>Data model: DynamoDB</h2>\n<p>As well as input files, the model also depends on a range of input parameters that the user can define. In the desktop app, the progam gets these input parameters by reading and parsing a local file called <code class=\"language-text\">INPUT_PARAMETERS.txt</code>.</p>\n<p>Each model run entity has a corresponding entity within a DynamoDB table (in practice there are multiple types of entity available available for the user to manipulate). The database follows the single-table design as described in <a href=\"https://www.dynamodbbook.com/\">The DynamoDB Handbook</a> by Alex BeBrie and uses <a href=\"https://github.com/jeremydaly/dynamodb-toolbox\"><code class=\"language-text\">dynamodb-toolbox</code></a> to model entities. The user can interact with these entities via the web application to set properties of the run.</p>\n<p>We created a corresponding manager in the dockerized model code for each type of entity in the database (e.g. <code class=\"language-text\">RunManager</code>) which can get the entity and also perform simple updates on text fields (set error messages etc.). The entity is retrieved from DynamoDB which then override the variables which were previously being retrieved from <code class=\"language-text\">INPUT_PARAMETERS.txt</code>, which otherwise describes a generic run.</p>\n<p>The model is now no-longer dependent on a local file for input of discrete variables; but how does the user configure those variables to get a final valid output to their desired parameters.</p>\n<h2>Web Application: Lambda + Next.js</h2>\n<p>It's now time to plug the main pieces together.</p>\n<p>We developed a Next.js frontend to interface with the remote model. This comes with all the advantages of developing UI using technologies that were designed to handle UI (i.e. not Python!). This meant our input methods were more interactive and our outputs were more visual, as we were able to leverage existing libraries.</p>\n<p>We added validation rules to the inputs (with warning messages) to prevent the user running the model with parameters that don't make sense before saving these parameters to DynamoDB. We accept input files as upload fields in a form, which had some pre-processing steps to validate the values within the files were within the restrictions before uploading to S3.</p>\n<p>We used AWS Lambdas to build our backend REST API. Serverless offers flexible scaling, has a nice integration to the back-office tool we created in Retool, and we didn't care about cold-starts as the model already took hours to run.</p>\n<p>Another advantage of using Lambda was that we can use AWS CloudFormation (infrastructure as code solution) to provision our infrastructure from a <code class=\"language-text\">serverless.ts</code> file which defines all the lambdas and resources we would need. If we wanted to create a new environment, running <code class=\"language-text\">sls deploy</code> would provision the resources defined in the file instead of needing to interact with the AWS console.</p>\n<h2>Notifying model outcomes: EventBridge + SES</h2>\n<p>Until now, a specific model task can be triggered to run in the cloud where it will sync the relevant input files, pull the correct input parameters, run the task, and sync the output files back to S3. Now we just need to notify the user of the outcome of the model (success + results/ error + logs) and show them the results.</p>\n<p>In the model, we defined a <code class=\"language-text\">RunEventsManager</code> which could send an event via AWS <a href=\"https://aws.amazon.com/eventbridge/\">EventBridge</a> when the <code class=\"language-text\">postprocess</code> or <code class=\"language-text\">failure</code> handler was completed in the model. We added a Lambda in our backend which was triggered by an eventbridge event (because we were using CloudFormation we just needed to define the event that triggers the lambda in <code class=\"language-text\">serverless.ts</code>).</p>\n<p>When an event occurs, the Lambda uses AWS <a href=\"https://aws.amazon.com/ses/\">SES</a> client to send an email to the user, which notifies them immediately when they can check their results - there's no need to come back to check that the model hasn't failed every 15 minutes anymore!</p>\n<h1>Conclusion</h1>\n<p>By the end of the project, we were running complex Python code (which required the use of awkward libraries, system calls, and file syncing) in the cloud. We could trigger a task in Fargate to run a model from our frontend. We can run as many tasks as the user requires concurrently and we will get notified by email of any updates to our runs which require our attention.</p>","frontmatter":{"date":"August 21, 2021","slug":"2021/08/migrating-desktop-apps-to-the-cloud","title":"Architecting a tricky cloud migration: how you can bring any software product to the cloud with AWS","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA9ElEQVQY04XMPS8DARzH8d/1JkQTU0Ni6yASJMaKxcLQyWpVEa/A0kRKhKG487B5Md4Bi8XDYGpET9z1/k+n/YuV4b7jd/gAf8rzAbIsxWfSHxORTSJqqOqyiCwx8xpT3ny+vwvfXh7xlXygNBUKlAdg5jkTigrTC2HuivC1MO1pMWqL+7QMHTr0oBQ098DcIfa9qO4tdV8RG26wjZqp+6yk77vae6hb/wmavJaDdAjwMZB3UJdTNGwL1d/v7rCbiRqdV7blBOvUBqiDcpAvx9E7qyG7nQ8tClYtDveLuHJUxOGBRdjhLhbSaHIquZpBFlf/AT+JUpai18VmwwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/821da/thumbnail.png","srcSet":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/22719/thumbnail.png 185w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/b91c6/thumbnail.png 370w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/821da/thumbnail.png 740w","sizes":"(min-width: 740px) 740px, 100vw"},"sources":[{"srcSet":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/6b17e/thumbnail.avif 185w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/097b8/thumbnail.avif 370w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/3950a/thumbnail.avif 740w","type":"image/avif","sizes":"(min-width: 740px) 740px, 100vw"},{"srcSet":"/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/4d5e6/thumbnail.webp 185w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/bb639/thumbnail.webp 370w,\n/blog/static/5b48b60e44e5a5f7dcb5dda78c830e8f/4baf0/thumbnail.webp 740w","type":"image/webp","sizes":"(min-width: 740px) 740px, 100vw"}]},"width":800,"height":196.75675675675677}}}}}},"pageContext":{"slug":"2021/08/migrating-desktop-apps-to-the-cloud"}},"staticQueryHashes":[]}